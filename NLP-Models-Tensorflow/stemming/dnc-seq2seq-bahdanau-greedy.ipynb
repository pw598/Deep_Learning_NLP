{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import itertools\n",
    "from dnc import DNC\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41760 41760\n"
     ]
    }
   ],
   "source": [
    "with open('lemmatization-en.txt','r') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "after, before = [], []\n",
    "for i in texts:\n",
    "    splitted = i.encode('ascii', 'ignore').decode(\"utf-8\").lower().split('\\t')\n",
    "    if len(splitted) < 2:\n",
    "        continue\n",
    "    after.append(list(splitted[0]))\n",
    "    before.append(list(splitted[1]))\n",
    "    \n",
    "print(len(after),len(before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 30\n",
      "Most common words [('e', 44346), ('s', 43522), ('i', 34153), ('r', 26409), ('n', 26150), ('a', 25045)]\n",
      "Sample data [21, 6, 7, 5, 10, 10, 4, 8, 10, 19] ['f', 'i', 'r', 's', 't', 't', 'e', 'n', 't', 'h']\n",
      "filtered vocab size: 34\n",
      "% of vocab used: 113.33%\n"
     ]
    }
   ],
   "source": [
    "concat_from = list(itertools.chain(*before))\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
    "print('filtered vocab size:',len(dictionary_from))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_from)/vocabulary_size_from,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 39\n",
      "Most common words [('e', 38012), ('r', 25251), ('a', 25006), ('i', 24222), ('t', 23468), ('o', 21275)]\n",
      "Sample data [32, 32, 31, 32, 31, 31, 32, 31, 31, 31] ['1', '1', '0', '1', '0', '0', '1', '0', '0', '0']\n",
      "filtered vocab size: 43\n",
      "% of vocab used: 110.26%\n"
     ]
    }
   ],
   "source": [
    "concat_to = list(itertools.chain(*after))\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab from size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])\n",
    "print('filtered vocab size:',len(dictionary_to))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_to)/vocabulary_size_to,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(after)):\n",
    "    after[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reads = 5\n",
    "num_writes = 1\n",
    "memory_size = 128\n",
    "word_size = 128\n",
    "clip_value = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 from_dict_size, to_dict_size, learning_rate, batch_size,\n",
    "                attn_input_feeding=True):\n",
    "        \n",
    "        def attn_decoder_input_fn(inputs, attention):\n",
    "            if attn_input_feeding:\n",
    "                return inputs\n",
    "        \n",
    "        def attention(encoder_out, cell, seq_len, encoder_last_state, reuse=False):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = size_layer, \n",
    "                                                                    memory = encoder_out,\n",
    "                                                                    memory_sequence_length = seq_len)\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = cell, \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = size_layer,\n",
    "                cell_input_fn=attn_decoder_input_fn,\n",
    "                initial_cell_state=encoder_last_state,\n",
    "                alignment_history=False)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        access_config = {\n",
    "            \"memory_size\": memory_size,\n",
    "            \"word_size\": word_size,\n",
    "            \"num_reads\": num_reads,\n",
    "            \"num_writes\": num_writes,\n",
    "        }\n",
    "        controller_config = {\n",
    "            \"hidden_size\": size_layer,\n",
    "        }\n",
    "        self.dnc_cell = DNC(access_config=access_config, controller_config=controller_config,\n",
    "                            output_size=size_layer, clip_value=clip_value)\n",
    "        self.dnc_cell_dec = DNC(access_config=access_config, controller_config=controller_config,\n",
    "                            output_size=size_layer, clip_value=clip_value)\n",
    "        self.dnc_initial = self.dnc_cell.initial_state\n",
    "        \n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        \n",
    "        initial_state = self.dnc_initial(batch_size)\n",
    "        self.encoder_out, self.encoder_state = tf.nn.dynamic_rnn(\n",
    "            cell=self.dnc_cell, inputs=encoder_embedded,\n",
    "            sequence_length=self.X_seq_len, dtype=tf.float32,\n",
    "            initial_state=initial_state)\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        # decoder\n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        decoder_cell = attention(self.encoder_out, self.dnc_cell_dec, self.X_seq_len,self.encoder_state)\n",
    "        dense_layer = tf.layers.Dense(to_dict_size)\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
    "            sequence_length = self.Y_seq_len,\n",
    "            time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = decoder_cell,\n",
    "            helper = training_helper,\n",
    "            initial_state = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32),\n",
    "            output_layer = dense_layer)\n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = training_decoder,\n",
    "            impute_finished = True,\n",
    "            output_time_major=False,\n",
    "            maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        \n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding = decoder_embeddings,\n",
    "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                end_token = EOS)\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = decoder_cell,\n",
    "                helper = predicting_helper,\n",
    "                initial_state = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32),\n",
    "                output_layer = dense_layer)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = predicting_decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = 2 * tf.reduce_max(self.X_seq_len))\n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.predicting_ids = predicting_decoder_output.sample_id\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 2\n",
    "embedded_size = 32\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sonnet nest is deprecated. Please use tf.contrib.framework.nest instead. In addition, `map` is renamed to `map_structure`.\n",
      "WARNING:tensorflow:From /home/barbatos/Desktop/NLP-Models/Stemming/addressing.py:35: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Stemmer(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
    "                len(dictionary_to), learning_rate,batch_size)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i:\n",
    "            try:\n",
    "                ints.append(dic[k])\n",
    "            except Exception as e:\n",
    "                ints.append(UNK)\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(before, dictionary_from)\n",
    "Y = str_idx(after, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n",
    "\n",
    "def check_accuracy(logits, Y):\n",
    "    acc = 0\n",
    "    for i in range(logits.shape[0]):\n",
    "        internal_acc = 0\n",
    "        count = 0\n",
    "        for k in range(len(Y[i])):\n",
    "            try:\n",
    "                if Y[i][k] == logits[i][k]:\n",
    "                    internal_acc += 1\n",
    "                count += 1\n",
    "                if Y[i][k] == EOS:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "        acc += (internal_acc / count)\n",
    "    return acc / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 0.564102, avg accuracy: 0.799230\n",
      "epoch: 1, avg valid loss: 0.159018, avg valid accuracy: 0.927903\n",
      "\n",
      "epoch: 2, avg loss: 0.104962, avg accuracy: 0.956576\n",
      "epoch: 2, avg valid loss: 0.096548, avg valid accuracy: 0.964121\n",
      "\n",
      "epoch: 3, avg loss: 0.071181, avg accuracy: 0.968311\n",
      "epoch: 3, avg valid loss: 0.057833, avg valid accuracy: 0.974550\n",
      "\n",
      "epoch: 4, avg loss: 0.062055, avg accuracy: 0.970528\n",
      "epoch: 4, avg valid loss: 0.061314, avg valid accuracy: 0.969753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy, total_accuracy_validation, total_loss_validation = 0, 0, 0, 0\n",
    "    for k in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x, seq_x = pad_sentence_batch(train_X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(train_Y[k: k+batch_size], PAD)\n",
    "        predicted, loss, _ = sess.run([model.predicting_ids, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})\n",
    "        total_loss += loss\n",
    "        total_accuracy += check_accuracy(predicted,batch_y)\n",
    "    total_loss /= (len(train_X) // batch_size)\n",
    "    total_accuracy /= (len(train_X) // batch_size)\n",
    "    for k in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x, seq_x = pad_sentence_batch(test_X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(test_Y[k: k+batch_size], PAD)\n",
    "        predicted, loss = sess.run([model.predicting_ids, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})\n",
    "        total_loss_validation += loss\n",
    "        total_accuracy_validation += check_accuracy(predicted,batch_y)\n",
    "    total_loss_validation /= (len(test_X) // batch_size)\n",
    "    total_accuracy_validation /= (len(test_X) // batch_size)\n",
    "    \n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))\n",
    "    print('epoch: %d, avg valid loss: %f, avg valid accuracy: %f\\n'%(i+1,total_loss_validation,\n",
    "                                                                  total_accuracy_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "batch_x, seq_x = pad_sentence_batch(test_X[k: k+batch_size], PAD)\n",
    "batch_y, seq_y = pad_sentence_batch(test_Y[k: k+batch_size], PAD)\n",
    "predicted, loss = sess.run([model.predicting_ids, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.X_seq_len:seq_x,\n",
    "                                                model.Y_seq_len:seq_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1\n",
      "BEFORE: kidnaped\n",
      "REAL AFTER: kidnap\n",
      "PREDICTED AFTER: kidnape \n",
      "\n",
      "row 2\n",
      "BEFORE: bizarrer\n",
      "REAL AFTER: bizarre\n",
      "PREDICTED AFTER: bizar \n",
      "\n",
      "row 3\n",
      "BEFORE: monologs\n",
      "REAL AFTER: monolog\n",
      "PREDICTED AFTER: monolog \n",
      "\n",
      "row 4\n",
      "BEFORE: synthesized\n",
      "REAL AFTER: synthesize\n",
      "PREDICTED AFTER: synthesize \n",
      "\n",
      "row 5\n",
      "BEFORE: amazons\n",
      "REAL AFTER: amazon\n",
      "PREDICTED AFTER: amazon \n",
      "\n",
      "row 6\n",
      "BEFORE: beckons\n",
      "REAL AFTER: beckon\n",
      "PREDICTED AFTER: beckon \n",
      "\n",
      "row 7\n",
      "BEFORE: re-interpretations\n",
      "REAL AFTER: re-interpretation\n",
      "PREDICTED AFTER: re-interpretation \n",
      "\n",
      "row 8\n",
      "BEFORE: jack-knifes\n",
      "REAL AFTER: jack-knife\n",
      "PREDICTED AFTER: jack-knife \n",
      "\n",
      "row 9\n",
      "BEFORE: appropriates\n",
      "REAL AFTER: appropriate\n",
      "PREDICTED AFTER: appropriate \n",
      "\n",
      "row 10\n",
      "BEFORE: cosignatories\n",
      "REAL AFTER: cosignatory\n",
      "PREDICTED AFTER: cosignatory \n",
      "\n",
      "row 11\n",
      "BEFORE: highballs\n",
      "REAL AFTER: highball\n",
      "PREDICTED AFTER: highbal \n",
      "\n",
      "row 12\n",
      "BEFORE: curing\n",
      "REAL AFTER: cure\n",
      "PREDICTED AFTER: cure \n",
      "\n",
      "row 13\n",
      "BEFORE: staidest\n",
      "REAL AFTER: staid\n",
      "PREDICTED AFTER: staid \n",
      "\n",
      "row 14\n",
      "BEFORE: lecterns\n",
      "REAL AFTER: lectern\n",
      "PREDICTED AFTER: lectern \n",
      "\n",
      "row 15\n",
      "BEFORE: least\n",
      "REAL AFTER: little\n",
      "PREDICTED AFTER: lea \n",
      "\n",
      "row 16\n",
      "BEFORE: floured\n",
      "REAL AFTER: flour\n",
      "PREDICTED AFTER: flour \n",
      "\n",
      "row 17\n",
      "BEFORE: shoveling\n",
      "REAL AFTER: shovel\n",
      "PREDICTED AFTER: shovel \n",
      "\n",
      "row 18\n",
      "BEFORE: weenier\n",
      "REAL AFTER: weeny\n",
      "PREDICTED AFTER: weeny \n",
      "\n",
      "row 19\n",
      "BEFORE: violas\n",
      "REAL AFTER: viola\n",
      "PREDICTED AFTER: viola \n",
      "\n",
      "row 20\n",
      "BEFORE: wanked\n",
      "REAL AFTER: wank\n",
      "PREDICTED AFTER: wank \n",
      "\n",
      "row 21\n",
      "BEFORE: blasts\n",
      "REAL AFTER: blast\n",
      "PREDICTED AFTER: blast \n",
      "\n",
      "row 22\n",
      "BEFORE: pegging\n",
      "REAL AFTER: peg\n",
      "PREDICTED AFTER: peg \n",
      "\n",
      "row 23\n",
      "BEFORE: pacesetters\n",
      "REAL AFTER: pacesetter\n",
      "PREDICTED AFTER: pacesetter \n",
      "\n",
      "row 24\n",
      "BEFORE: honking\n",
      "REAL AFTER: honk\n",
      "PREDICTED AFTER: honk \n",
      "\n",
      "row 25\n",
      "BEFORE: prizes\n",
      "REAL AFTER: prize\n",
      "PREDICTED AFTER: prize \n",
      "\n",
      "row 26\n",
      "BEFORE: shipyards\n",
      "REAL AFTER: shipyard\n",
      "PREDICTED AFTER: shipyard \n",
      "\n",
      "row 27\n",
      "BEFORE: tomes\n",
      "REAL AFTER: tome\n",
      "PREDICTED AFTER: tome \n",
      "\n",
      "row 28\n",
      "BEFORE: frizzling\n",
      "REAL AFTER: frizzle\n",
      "PREDICTED AFTER: frizzle \n",
      "\n",
      "row 29\n",
      "BEFORE: showmen\n",
      "REAL AFTER: showman\n",
      "PREDICTED AFTER: showman \n",
      "\n",
      "row 30\n",
      "BEFORE: reinstalling\n",
      "REAL AFTER: reinstall\n",
      "PREDICTED AFTER: reinstal \n",
      "\n",
      "row 31\n",
      "BEFORE: daughters-in-law\n",
      "REAL AFTER: daughter-in-law\n",
      "PREDICTED AFTER: daughter-inlaw \n",
      "\n",
      "row 32\n",
      "BEFORE: calmer\n",
      "REAL AFTER: calm\n",
      "PREDICTED AFTER: calm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('BEFORE:',''.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
    "    print('REAL AFTER:',''.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
    "    print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
