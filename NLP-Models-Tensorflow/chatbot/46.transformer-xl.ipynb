{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        \n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "    \n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    return ' '.join([i.strip() for i in filter(None, text.split())])\n",
    "\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))\n",
    "    \n",
    "min_line_length = 2\n",
    "max_line_length = 5\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1\n",
    "    \n",
    "question_test = short_questions[500:550]\n",
    "answer_test = short_answers[500:550]\n",
    "short_questions = short_questions[:500]\n",
    "short_answers = short_answers[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 657\n",
      "Most common words [('you', 132), ('is', 78), ('i', 68), ('what', 51), ('it', 50), ('that', 49)]\n",
      "Sample data [7, 28, 129, 35, 61, 42, 12, 22, 82, 225] ['what', 'good', 'stuff', 'she', 'okay', 'they', 'do', 'to', 'hey', 'sweet']\n",
      "filtered vocab size: 661\n",
      "% of vocab used: 100.61%\n"
     ]
    }
   ],
   "source": [
    "concat_from = ' '.join(short_questions+question_test).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
    "print('filtered vocab size:',len(dictionary_from))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_from)/vocabulary_size_from,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 660\n",
      "Most common words [('i', 97), ('you', 91), ('is', 62), ('it', 58), ('not', 47), ('what', 39)]\n",
      "Sample data [12, 216, 5, 4, 94, 25, 59, 10, 8, 79] ['the', 'real', 'you', 'i', 'hope', 'so', 'they', 'do', 'not', 'hi']\n",
      "filtered vocab size: 664\n",
      "% of vocab used: 100.61%\n"
     ]
    }
   ],
   "source": [
    "concat_to = ' '.join(short_answers+answer_test).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab from size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])\n",
    "print('filtered vocab size:',len(dictionary_to))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_to)/vocabulary_size_to,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            ints.append(dic.get(k,UNK))\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int, maxlen):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = maxlen\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(maxlen)\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(short_questions, dictionary_from)\n",
    "Y = str_idx(short_answers, dictionary_to)\n",
    "X_test = str_idx(question_test, dictionary_from)\n",
    "Y_test = str_idx(answer_test, dictionary_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_question = max([len(x) for x in X]) * 2\n",
    "maxlen_answer = max([len(y) for y in Y]) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 16\n",
    "epoch = 20\n",
    "n_layer = 3\n",
    "d_model = 256\n",
    "d_embed = 256\n",
    "n_head = 10\n",
    "d_head = 50\n",
    "d_inner = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(pos_seq, inv_freq, bsz = None):\n",
    "    sinusoid_inp = tf.einsum('i,j->ij', pos_seq, inv_freq)\n",
    "    pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
    "    if bsz is not None:\n",
    "        return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
    "    else:\n",
    "        return pos_emb[:, None, :]\n",
    "\n",
    "\n",
    "def positionwise_FF(inp, d_model, d_inner, kernel_initializer, scope = 'ff'):\n",
    "    output = inp\n",
    "    with tf.variable_scope(scope):\n",
    "        output = tf.layers.dense(\n",
    "            inp,\n",
    "            d_inner,\n",
    "            activation = tf.nn.relu,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            name = 'layer_1',\n",
    "        )\n",
    "        output = tf.layers.dense(\n",
    "            output,\n",
    "            d_model,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            name = 'layer_2',\n",
    "        )\n",
    "        output = tf.contrib.layers.layer_norm(\n",
    "            output + inp, begin_norm_axis = -1\n",
    "        )\n",
    "    return output\n",
    "\n",
    "\n",
    "def rel_shift(x):\n",
    "    x_size = tf.shape(x)\n",
    "\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, x_size)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def rel_multihead_attn(\n",
    "    w,\n",
    "    r,\n",
    "    r_w_bias,\n",
    "    r_r_bias,\n",
    "    attn_mask,\n",
    "    mems,\n",
    "    d_model,\n",
    "    n_head,\n",
    "    d_head,\n",
    "    kernel_initializer,\n",
    "    scope = 'rel_attn',\n",
    "):\n",
    "    scale = 1 / (d_head ** 0.5)\n",
    "    with tf.variable_scope(scope):\n",
    "        qlen = tf.shape(w)[0]\n",
    "        rlen = tf.shape(r)[0]\n",
    "        bsz = tf.shape(w)[1]\n",
    "\n",
    "        cat = (\n",
    "            tf.concat([mems, w], 0)\n",
    "            if mems is not None and mems.shape.ndims > 1\n",
    "            else w\n",
    "        )\n",
    "        w_heads = tf.layers.dense(\n",
    "            cat,\n",
    "            3 * n_head * d_head,\n",
    "            use_bias = False,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            name = 'qkv',\n",
    "        )\n",
    "        r_head_k = tf.layers.dense(\n",
    "            r,\n",
    "            n_head * d_head,\n",
    "            use_bias = False,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            name = 'r',\n",
    "        )\n",
    "\n",
    "        w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, -1)\n",
    "        w_head_q = w_head_q[-qlen:]\n",
    "\n",
    "        klen = tf.shape(w_head_k)[0]\n",
    "\n",
    "        w_head_q = tf.reshape(w_head_q, [qlen, bsz, n_head, d_head])\n",
    "        w_head_k = tf.reshape(w_head_k, [klen, bsz, n_head, d_head])\n",
    "        w_head_v = tf.reshape(w_head_v, [klen, bsz, n_head, d_head])\n",
    "\n",
    "        r_head_k = tf.reshape(r_head_k, [rlen, n_head, d_head])\n",
    "\n",
    "        rw_head_q = w_head_q + r_w_bias\n",
    "        rr_head_q = w_head_q + r_r_bias\n",
    "\n",
    "        AC = tf.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)\n",
    "        BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)\n",
    "        BD = rel_shift(BD)\n",
    "        \n",
    "        paddings = tf.fill(tf.shape(BD), float('-inf'))\n",
    "\n",
    "        attn_score = (AC + BD) * scale\n",
    "        attn_mask_t = attn_mask[:, :, None, None]\n",
    "        attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
    "\n",
    "        attn_prob = tf.nn.softmax(attn_score, 1)\n",
    "        attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, w_head_v)\n",
    "        size_t = tf.shape(attn_vec)\n",
    "        attn_vec = tf.reshape(attn_vec, [size_t[0], size_t[1], n_head * d_head])\n",
    "\n",
    "        attn_out = tf.layers.dense(\n",
    "            attn_vec,\n",
    "            d_model,\n",
    "            use_bias = False,\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            name = 'o',\n",
    "        )\n",
    "\n",
    "        output = tf.contrib.layers.layer_norm(\n",
    "            attn_out + w, begin_norm_axis = -1\n",
    "        )\n",
    "    return output\n",
    "\n",
    "\n",
    "def embedding_lookup(lookup_table, x):\n",
    "    return tf.nn.embedding_lookup(lookup_table, x)\n",
    "\n",
    "\n",
    "def mask_adaptive_embedding_lookup(\n",
    "    x,\n",
    "    n_token,\n",
    "    d_embed,\n",
    "    d_proj,\n",
    "    cutoffs,\n",
    "    initializer,\n",
    "    proj_initializer,\n",
    "    div_val = 1,\n",
    "    proj_same_dim = True,\n",
    "    scope = 'adaptive_embed',\n",
    "    **kwargs\n",
    "):\n",
    "    emb_scale = d_proj ** 0.5\n",
    "    with tf.variable_scope(scope):\n",
    "        if div_val == 1:\n",
    "            lookup_table = tf.get_variable(\n",
    "                'lookup_table', [n_token, d_embed], initializer = initializer\n",
    "            )\n",
    "            y = embedding_lookup(lookup_table, x)\n",
    "            if d_proj != d_embed:\n",
    "                proj_W = tf.get_variable(\n",
    "                    'proj_W', [d_embed, d_proj], initializer = proj_initializer\n",
    "                )\n",
    "                y = tf.einsum('ibe,ed->ibd', y, proj_W)\n",
    "            else:\n",
    "                proj_W = None\n",
    "            ret_params = [lookup_table, proj_W]\n",
    "        else:\n",
    "            tables, projs = [], []\n",
    "            cutoff_ends = [0] + cutoffs + [n_token]\n",
    "            x_size = tf.shape(x)\n",
    "            y = tf.zeros([x_size[0], x_size[1], d_proj])\n",
    "            for i in range(len(cutoff_ends) - 1):\n",
    "                with tf.variable_scope('cutoff_{}'.format(i)):\n",
    "                    l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n",
    "                    mask = (x >= l_idx) & (x < r_idx)\n",
    "                    cur_x = tf.boolean_mask(x, mask) - l_idx\n",
    "                    cur_d_embed = d_embed // (div_val ** i)\n",
    "                    lookup_table = tf.get_variable(\n",
    "                        'lookup_table',\n",
    "                        [r_idx - l_idx, cur_d_embed],\n",
    "                        initializer = initializer,\n",
    "                    )\n",
    "                    cur_y = embedding_lookup(lookup_table, cur_x)\n",
    "                    if d_proj == cur_d_embed and not proj_same_dim:\n",
    "                        proj_W = None\n",
    "                    else:\n",
    "                        proj_W = tf.get_variable(\n",
    "                            'proj_W',\n",
    "                            [cur_d_embed, d_proj],\n",
    "                            initializer = proj_initializer,\n",
    "                        )\n",
    "                        cur_y = tf.einsum('id,de->ie', cur_y, proj_W)\n",
    "                    mask_idx = tf.to_int64(tf.where(mask))\n",
    "                    y += tf.scatter_nd(\n",
    "                        mask_idx, cur_y, tf.to_int64(tf.shape(y))\n",
    "                    )\n",
    "                    tables.append(lookup_table)\n",
    "                    projs.append(proj_W)\n",
    "            ret_params = [tables, projs]\n",
    "\n",
    "    y *= emb_scale\n",
    "    return y, ret_params\n",
    "\n",
    "\n",
    "def mul_adaptive_embedding_lookup(\n",
    "    x,\n",
    "    n_token,\n",
    "    d_embed,\n",
    "    d_proj,\n",
    "    cutoffs,\n",
    "    initializer,\n",
    "    proj_initializer,\n",
    "    div_val = 1,\n",
    "    perms = None,\n",
    "    proj_same_dim = True,\n",
    "    scope = 'adaptive_embed',\n",
    "):\n",
    "    \"\"\"\n",
    "  perms: If None, first compute W = W1 x W2 (projection for each bin),\n",
    "      and then compute X x W (embedding lookup). If not None,\n",
    "      use bin-based embedding lookup with max_bin_size defined by\n",
    "      the shape of perms.\n",
    "  \"\"\"\n",
    "    emb_scale = d_proj ** 0.5\n",
    "    with tf.variable_scope(scope):\n",
    "        if div_val == 1:\n",
    "            lookup_table = tf.get_variable(\n",
    "                'lookup_table', [n_token, d_embed], initializer = initializer\n",
    "            )\n",
    "            y = embedding_lookup(lookup_table, x)\n",
    "            if d_proj != d_embed:\n",
    "                proj_W = tf.get_variable(\n",
    "                    'proj_W', [d_embed, d_proj], initializer = proj_initializer\n",
    "                )\n",
    "                y = tf.einsum('ibe,ed->ibd', y, proj_W)\n",
    "            else:\n",
    "                proj_W = None\n",
    "            ret_params = [lookup_table, proj_W]\n",
    "        else:\n",
    "            tables, projs = [], []\n",
    "            cutoff_ends = [0] + cutoffs + [n_token]\n",
    "            x_size = tf.shape(x)\n",
    "            if perms is None:\n",
    "                cat_lookup = []\n",
    "            else:\n",
    "                cat_lookup = tf.zeros([x_size[0], x_size[1], d_proj])\n",
    "            for i in range(len(cutoff_ends) - 1):\n",
    "                with tf.variable_scope('cutoff_{}'.format(i)):\n",
    "                    l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n",
    "                    cur_d_embed = d_embed // (div_val ** i)\n",
    "                    lookup_table = tf.get_variable(\n",
    "                        'lookup_table',\n",
    "                        [r_idx - l_idx, cur_d_embed],\n",
    "                        initializer = initializer,\n",
    "                    )\n",
    "                    if cur_d_embed == d_proj and not proj_same_dim:\n",
    "                        proj_W = None\n",
    "                    else:\n",
    "                        proj_W = tf.get_variable(\n",
    "                            'proj_W',\n",
    "                            [cur_d_embed, d_proj],\n",
    "                            initializer = proj_initializer,\n",
    "                        )\n",
    "                    if perms is None:\n",
    "                        cat_lookup.append(\n",
    "                            tf.einsum('ie,ed->id', lookup_table, proj_W)\n",
    "                        )\n",
    "                    else:\n",
    "                        # speed up the computation of the first bin\n",
    "                        # also save some meory\n",
    "                        if i == 0:\n",
    "                            cur_y = embedding_lookup(\n",
    "                                lookup_table, tf.minimum(x, r_idx - 1)\n",
    "                            )\n",
    "                            if proj_W is not None:\n",
    "                                cur_y = tf.einsum('ibe,ed->ibd', cur_y, proj_W)\n",
    "                            cur_y *= perms[i][:, :, None]\n",
    "                            cat_lookup += cur_y\n",
    "                        else:\n",
    "                            cur_x = tf.einsum(\n",
    "                                'ib,ibk->k', tf.to_float(x - l_idx), perms[i]\n",
    "                            )\n",
    "                            cur_x = tf.to_int32(cur_x)\n",
    "                            cur_y = embedding_lookup(lookup_table, cur_x)\n",
    "                            if proj_W is not None:\n",
    "                                cur_y = tf.einsum('ke,ed->kd', cur_y, proj_W)\n",
    "                            cat_lookup += tf.einsum(\n",
    "                                'kd,ibk->ibd', cur_y, perms[i]\n",
    "                            )\n",
    "                    tables.append(lookup_table)\n",
    "                    projs.append(proj_W)\n",
    "            if perms is None:\n",
    "                cat_lookup = tf.concat(cat_lookup, 0)\n",
    "                y = embedding_lookup(cat_lookup, x)\n",
    "            else:\n",
    "                y = cat_lookup\n",
    "            ret_params = [tables, projs]\n",
    "\n",
    "    y *= emb_scale\n",
    "    return y, ret_params\n",
    "\n",
    "\n",
    "def mask_adaptive_logsoftmax(\n",
    "    hidden,\n",
    "    target,\n",
    "    n_token,\n",
    "    d_embed,\n",
    "    d_proj,\n",
    "    cutoffs,\n",
    "    params,\n",
    "    tie_projs,\n",
    "    initializer = None,\n",
    "    proj_initializer = None,\n",
    "    div_val = 1,\n",
    "    scope = 'adaptive_softmax',\n",
    "    proj_same_dim = True,\n",
    "    return_mean = True,\n",
    "    **kwargs\n",
    "):\n",
    "    def _logit(x, W, b, proj):\n",
    "        y = x\n",
    "        if proj is not None:\n",
    "            y = tf.einsum('ibd,ed->ibe', y, proj)\n",
    "        return tf.einsum('ibd,nd->ibn', y, W) + b\n",
    "\n",
    "    params_W, params_projs = params[0], params[1]\n",
    "\n",
    "    def _gather_logprob(logprob, target):\n",
    "        lp_size = tf.shape(logprob)\n",
    "        r = tf.range(lp_size[0])\n",
    "        idx = tf.stack([r, target], 1)\n",
    "        return tf.gather_nd(logprob, idx)\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        if len(cutoffs) == 0:\n",
    "            softmax_b = tf.get_variable(\n",
    "                'bias', [n_token], initializer = tf.zeros_initializer()\n",
    "            )\n",
    "            output = _logit(hidden, params_W, softmax_b, params_projs)\n",
    "            nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = target, logits = output\n",
    "            )\n",
    "        else:\n",
    "            cutoff_ends = [0] + cutoffs + [n_token]\n",
    "            nll = tf.zeros_like(target, dtype = tf.float32)\n",
    "            for i in range(len(cutoff_ends) - 1):\n",
    "                with tf.variable_scope('cutoff_{}'.format(i)):\n",
    "                    l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n",
    "                    mask = (target >= l_idx) & (target < r_idx)\n",
    "                    mask_idx = tf.where(mask)\n",
    "                    cur_target = tf.boolean_mask(target, mask) - l_idx\n",
    "                    cur_d_embed = d_embed // (div_val ** i)\n",
    "\n",
    "                    if div_val == 1:\n",
    "                        cur_W = params_W[l_idx:r_idx]\n",
    "                    else:\n",
    "                        cur_W = params_W[i]\n",
    "                    cur_b = tf.get_variable(\n",
    "                        'b',\n",
    "                        [r_idx - l_idx],\n",
    "                        initializer = tf.zeros_initializer(),\n",
    "                    )\n",
    "                    if tie_projs[i]:\n",
    "                        if div_val == 1:\n",
    "                            cur_proj = params_projs\n",
    "                        else:\n",
    "                            cur_proj = params_projs[i]\n",
    "                    else:\n",
    "                        if (\n",
    "                            div_val == 1 or not proj_same_dim\n",
    "                        ) and d_proj == cur_d_embed:\n",
    "                            cur_proj = None\n",
    "                        else:\n",
    "                            cur_proj = tf.get_variable(\n",
    "                                'proj',\n",
    "                                [cur_d_embed, d_proj],\n",
    "                                initializer = proj_initializer,\n",
    "                            )\n",
    "                    if i == 0:\n",
    "                        cluster_W = tf.get_variable(\n",
    "                            'cluster_W',\n",
    "                            [len(cutoffs), d_embed],\n",
    "                            initializer = tf.zeros_initializer(),\n",
    "                        )\n",
    "                        cluster_b = tf.get_variable(\n",
    "                            'cluster_b',\n",
    "                            [len(cutoffs)],\n",
    "                            initializer = tf.zeros_initializer(),\n",
    "                        )\n",
    "                        cur_W = tf.concat([cur_W, cluster_W], 0)\n",
    "                        cur_b = tf.concat([cur_b, cluster_b], 0)\n",
    "\n",
    "                        head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n",
    "                        head_logprob = tf.nn.log_softmax(head_logit)\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_logprob = _gather_logprob(\n",
    "                            cur_head_logprob, cur_target\n",
    "                        )\n",
    "                    else:\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_hidden = tf.boolean_mask(hidden, mask)\n",
    "                        tail_logit = tf.squeeze(\n",
    "                            _logit(cur_hidden[None], cur_W, cur_b, cur_proj), 0\n",
    "                        )\n",
    "                        tail_logprob = tf.nn.log_softmax(tail_logit)\n",
    "                        cur_logprob = cur_head_logprob[\n",
    "                            :, cutoff_ends[1] + i - 1\n",
    "                        ] + _gather_logprob(tail_logprob, cur_target)\n",
    "                    nll += tf.scatter_nd(\n",
    "                        mask_idx, -cur_logprob, tf.to_int64(tf.shape(nll))\n",
    "                    )\n",
    "    if return_mean:\n",
    "        nll = tf.reduce_mean(nll)\n",
    "    return nll\n",
    "\n",
    "\n",
    "def mul_adaptive_logsoftmax(\n",
    "    hidden,\n",
    "    target,\n",
    "    n_token,\n",
    "    d_embed,\n",
    "    d_proj,\n",
    "    cutoffs,\n",
    "    params,\n",
    "    tie_projs,\n",
    "    initializer = None,\n",
    "    proj_initializer = None,\n",
    "    div_val = 1,\n",
    "    perms = None,\n",
    "    proj_same_dim = True,\n",
    "    scope = 'adaptive_softmax',\n",
    "    **kwargs\n",
    "):\n",
    "    def _logit(x, W, b, proj):\n",
    "        y = x\n",
    "        if x.shape.ndims == 3:\n",
    "            if proj is not None:\n",
    "                y = tf.einsum('ibd,ed->ibe', y, proj)\n",
    "            return tf.einsum('ibd,nd->ibn', y, W) + b\n",
    "        else:\n",
    "            if proj is not None:\n",
    "                y = tf.einsum('id,ed->ie', y, proj)\n",
    "            return tf.einsum('id,nd->in', y, W) + b\n",
    "\n",
    "    params_W, params_projs = params[0], params[1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        if len(cutoffs) == 0:\n",
    "            softmax_b = tf.get_variable(\n",
    "                'bias', [n_token], initializer = tf.zeros_initializer()\n",
    "            )\n",
    "            output = _logit(hidden, params_W, softmax_b, params_projs)\n",
    "            nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = target, logits = output\n",
    "            )\n",
    "            nll = tf.reduce_mean(nll)\n",
    "        else:\n",
    "            total_loss, total_cnt = 0, 0\n",
    "            cutoff_ends = [0] + cutoffs + [n_token]\n",
    "            for i in range(len(cutoff_ends) - 1):\n",
    "                with tf.variable_scope('cutoff_{}'.format(i)):\n",
    "                    l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n",
    "\n",
    "                    cur_d_embed = d_embed // (div_val ** i)\n",
    "\n",
    "                    if div_val == 1:\n",
    "                        cur_W = params_W[l_idx:r_idx]\n",
    "                    else:\n",
    "                        cur_W = params_W[i]\n",
    "                    cur_b = tf.get_variable(\n",
    "                        'b',\n",
    "                        [r_idx - l_idx],\n",
    "                        initializer = tf.zeros_initializer(),\n",
    "                    )\n",
    "                    if tie_projs[i]:\n",
    "                        if div_val == 1:\n",
    "                            cur_proj = params_projs\n",
    "                        else:\n",
    "                            cur_proj = params_projs[i]\n",
    "                    else:\n",
    "                        if (\n",
    "                            div_val == 1 or not proj_same_dim\n",
    "                        ) and d_proj == cur_d_embed:\n",
    "                            cur_proj = None\n",
    "                        else:\n",
    "                            cur_proj = tf.get_variable(\n",
    "                                'proj',\n",
    "                                [cur_d_embed, d_proj],\n",
    "                                initializer = proj_initializer,\n",
    "                            )\n",
    "\n",
    "                    if i == 0:\n",
    "                        cluster_W = tf.get_variable(\n",
    "                            'cluster_W',\n",
    "                            [len(cutoffs), d_embed],\n",
    "                            initializer = tf.zeros_initializer(),\n",
    "                        )\n",
    "                        cluster_b = tf.get_variable(\n",
    "                            'cluster_b',\n",
    "                            [len(cutoffs)],\n",
    "                            initializer = tf.zeros_initializer(),\n",
    "                        )\n",
    "                        cur_W = tf.concat([cur_W, cluster_W], 0)\n",
    "                        cur_b = tf.concat([cur_b, cluster_b], 0)\n",
    "\n",
    "                        head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n",
    "\n",
    "                        head_target = kwargs.get('head_target')\n",
    "                        head_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                            labels = head_target, logits = head_logit\n",
    "                        )\n",
    "\n",
    "                        masked_loss = head_nll * perms[i]\n",
    "                        total_loss += tf.reduce_sum(masked_loss)\n",
    "                        total_cnt += tf.reduce_sum(perms[i])\n",
    "\n",
    "                        # head_logprob = tf.nn.log_softmax(head_logit)\n",
    "\n",
    "                        # final_logprob = head_logprob * perms[i][:, :, None]\n",
    "                        # final_target = tf.one_hot(target, tf.shape(head_logprob)[2])\n",
    "                        # total_loss -= tf.einsum('ibn,ibn->', final_logprob, final_target)\n",
    "                        # total_cnt += tf.reduce_sum(perms[i])\n",
    "                    else:\n",
    "                        cur_head_nll = tf.einsum(\n",
    "                            'ib,ibk->k', head_nll, perms[i]\n",
    "                        )\n",
    "\n",
    "                        cur_hidden = tf.einsum('ibd,ibk->kd', hidden, perms[i])\n",
    "                        tail_logit = _logit(cur_hidden, cur_W, cur_b, cur_proj)\n",
    "\n",
    "                        tail_target = tf.einsum(\n",
    "                            'ib,ibk->k', tf.to_float(target - l_idx), perms[i]\n",
    "                        )\n",
    "                        tail_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                            labels = tf.to_int32(tail_target),\n",
    "                            logits = tail_logit,\n",
    "                        )\n",
    "\n",
    "                        sum_nll = cur_head_nll + tail_nll\n",
    "                        mask = tf.reduce_sum(perms[i], [0, 1])\n",
    "\n",
    "                        masked_loss = sum_nll * mask\n",
    "                        total_loss += tf.reduce_sum(masked_loss)\n",
    "                        total_cnt += tf.reduce_sum(mask)\n",
    "\n",
    "            nll = total_loss / total_cnt\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "def _create_mask(qlen, mlen, same_length = False):\n",
    "    attn_mask = tf.ones([qlen, qlen])\n",
    "    mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n",
    "    mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n",
    "    attn_mask_pad = tf.zeros([qlen, mlen])\n",
    "    ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
    "    if same_length:\n",
    "        mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
    "        ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _cache_mem(curr_out, prev_mem, mem_len = None):\n",
    "    if mem_len is None or prev_mem is None:\n",
    "        new_mem = curr_out\n",
    "    elif mem_len == 0:\n",
    "        return prev_mem\n",
    "    else:\n",
    "        new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)\n",
    "\n",
    "\n",
    "def transformer(\n",
    "    dec_inp,\n",
    "    mems,\n",
    "    n_token,\n",
    "    n_layer,\n",
    "    d_model,\n",
    "    d_embed,\n",
    "    n_head,\n",
    "    d_head,\n",
    "    d_inner,\n",
    "    initializer,\n",
    "    proj_initializer = None,\n",
    "    mem_len = None,\n",
    "    cutoffs = [],\n",
    "    div_val = 1,\n",
    "    tie_projs = [],\n",
    "    same_length = False,\n",
    "    clamp_len = -1,\n",
    "    untie_r = False,\n",
    "    proj_same_dim = True,\n",
    "    scope = 'transformer',\n",
    "    reuse = tf.AUTO_REUSE\n",
    "):\n",
    "    \"\"\"\n",
    "  cutoffs: a list of python int. Cutoffs for adaptive softmax.\n",
    "  tie_projs: a list of python bools. Whether to tie the projections.\n",
    "  perms: a list of tensors. Each tensor should of size [len, bsz, bin_size].\n",
    "        Only used in the adaptive setting.\n",
    "  \"\"\"\n",
    "    new_mems = []\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        if untie_r:\n",
    "            r_w_bias = tf.get_variable(\n",
    "                'r_w_bias', [n_layer, n_head, d_head], initializer = initializer\n",
    "            )\n",
    "            r_r_bias = tf.get_variable(\n",
    "                'r_r_bias', [n_layer, n_head, d_head], initializer = initializer\n",
    "            )\n",
    "        else:\n",
    "            r_w_bias = tf.get_variable(\n",
    "                'r_w_bias', [n_head, d_head], initializer = initializer\n",
    "            )\n",
    "            r_r_bias = tf.get_variable(\n",
    "                'r_r_bias', [n_head, d_head], initializer = initializer\n",
    "            )\n",
    "\n",
    "        qlen = tf.shape(dec_inp)[0]\n",
    "        mlen = tf.shape(mems[0])[0] if mems is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        if proj_initializer is None:\n",
    "            proj_initializer = initializer\n",
    "        lookup_fn = mask_adaptive_embedding_lookup\n",
    "        embeddings, shared_params = lookup_fn(\n",
    "            x = dec_inp,\n",
    "            n_token = n_token,\n",
    "            d_embed = d_embed,\n",
    "            d_proj = d_model,\n",
    "            cutoffs = cutoffs,\n",
    "            initializer = initializer,\n",
    "            proj_initializer = proj_initializer,\n",
    "            div_val = div_val,\n",
    "            proj_same_dim = proj_same_dim,\n",
    "        )\n",
    "\n",
    "        attn_mask = _create_mask(qlen, mlen, same_length)\n",
    "\n",
    "        pos_seq = tf.range(klen - 1, -1, -1.0)\n",
    "        if clamp_len > 0:\n",
    "            pos_seq = tf.minimum(pos_seq, clamp_len)\n",
    "        inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n",
    "        pos_emb = positional_embedding(pos_seq, inv_freq)\n",
    "\n",
    "        if mems is None:\n",
    "            mems = [None] * n_layer\n",
    "        output = embeddings\n",
    "        for i in range(n_layer):\n",
    "            # cache new mems\n",
    "            new_mems.append(_cache_mem(output, mems[i], mem_len))\n",
    "\n",
    "            with tf.variable_scope('layer_{}'.format(i)):\n",
    "                output = rel_multihead_attn(\n",
    "                    w = output,\n",
    "                    r = pos_emb,\n",
    "                    r_w_bias = r_w_bias if not untie_r else r_w_bias[i],\n",
    "                    r_r_bias = r_r_bias if not untie_r else r_r_bias[i],\n",
    "                    attn_mask = attn_mask,\n",
    "                    mems = mems[i],\n",
    "                    d_model = d_model,\n",
    "                    n_head = n_head,\n",
    "                    d_head = d_head,\n",
    "                    kernel_initializer = initializer,\n",
    "                )\n",
    "                output = positionwise_FF(\n",
    "                    inp = output,\n",
    "                    d_model = d_model,\n",
    "                    d_inner = d_inner,\n",
    "                    kernel_initializer = initializer,\n",
    "                )\n",
    "\n",
    "        return output, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype = tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype = tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        initializer = tf.initializers.random_normal(stddev = 0.1)\n",
    "\n",
    "        def forward(x, y, reuse = tf.AUTO_REUSE):\n",
    "            memory = tf.fill(\n",
    "                [n_layer, tf.shape(x)[0], tf.shape(x)[1], d_model], PAD\n",
    "            )\n",
    "            memory = tf.cast(memory, tf.float32)\n",
    "            logits, next_memory = transformer(\n",
    "                x,\n",
    "                memory,\n",
    "                len(dictionary_from),\n",
    "                n_layer,\n",
    "                d_model,\n",
    "                d_embed,\n",
    "                n_head,\n",
    "                d_head,\n",
    "                d_inner,\n",
    "                initializer,\n",
    "                scope='encoder',\n",
    "                reuse=reuse\n",
    "            )\n",
    "            logits, next_memory = transformer(\n",
    "                x,\n",
    "                next_memory,\n",
    "                len(dictionary_to),\n",
    "                n_layer,\n",
    "                d_model,\n",
    "                d_embed,\n",
    "                n_head,\n",
    "                d_head,\n",
    "                d_inner,\n",
    "                initializer,\n",
    "                scope='decoder',\n",
    "                reuse=reuse\n",
    "            )\n",
    "            logits = transformer(\n",
    "                y,\n",
    "                next_memory,\n",
    "                len(dictionary_to),\n",
    "                n_layer,\n",
    "                d_model,\n",
    "                d_embed,\n",
    "                n_head,\n",
    "                d_head,\n",
    "                d_inner,\n",
    "                initializer,\n",
    "                scope='decoder_1',\n",
    "                reuse=reuse\n",
    "            )[0]\n",
    "            return tf.layers.dense(logits, len(dictionary_from), reuse=tf.AUTO_REUSE)\n",
    "        self.training_logits = forward(self.X, decoder_input)\n",
    "        \n",
    "        def cond(i, y, temp):\n",
    "            return i < tf.reduce_max(tf.shape(self.X)[1])\n",
    "        \n",
    "        def body(i, y, temp):\n",
    "            logits = forward(self.X, y,reuse=True)\n",
    "            ids = tf.argmax(logits, -1)[:, i]\n",
    "            ids = tf.expand_dims(ids, -1)\n",
    "            temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "            y = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "            y = tf.reshape(y, [tf.shape(temp)[0], tf.shape(self.X)[1]])\n",
    "            i += 1\n",
    "            return i, y, temp\n",
    "        \n",
    "        target = tf.fill([batch_size, tf.shape(self.X)[1]], GO)\n",
    "        target = tf.cast(target, tf.int64)\n",
    "        self.target = target\n",
    "        \n",
    "        _, self.predicting_ids, _ = tf.while_loop(cond, body, \n",
    "                                                  [tf.constant(0), target, target])\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, maxlen_answer, dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Chatbot()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 1, avg loss: 5.827626, avg accuracy: 0.190200\n",
      "\n",
      "[[4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 2, avg loss: 4.959541, avg accuracy: 0.238764\n",
      "\n",
      "[[4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 3, avg loss: 4.537225, avg accuracy: 0.274584\n",
      "\n",
      "[[4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 4, avg loss: 4.149358, avg accuracy: 0.303502\n",
      "\n",
      "[[4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 5, avg loss: 3.817156, avg accuracy: 0.339181\n",
      "\n",
      "[[4 4 2 4 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 6, avg loss: 3.531980, avg accuracy: 0.370425\n",
      "\n",
      "[[4 5 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 8 2 2 2 2 2 2 2 2 2 2]\n",
      " [4 5 2 2 2 2 2 2 2 2 2 2]]\n",
      "epoch: 7, avg loss: 3.291441, avg accuracy: 0.402090\n",
      "\n",
      "[[ 4  4  2  9  2  2  2  2  2  2  2  2]\n",
      " [ 4  8  2  9  2  2  2  2  2  2  2  2]\n",
      " [ 4  8  2  9  2  2  2  2  2  2  2  2]\n",
      " [ 4  8 18  9  2  2  2  2  2  2  2  2]]\n",
      "epoch: 8, avg loss: 3.093053, avg accuracy: 0.460245\n",
      "\n",
      "[[ 4  4  2  4  2  2  2  2  2  2  2  2]\n",
      " [ 4  4  2  4  2  2  2  2  2  2  2  2]\n",
      " [ 4  8  2  4  2  2  2  2  2  2  2  2]\n",
      " [ 4  5 18  9  2  2  2  2  2  2  2  2]]\n",
      "epoch: 9, avg loss: 2.847723, avg accuracy: 0.496294\n",
      "\n",
      "[[4 4 2 4 2 2 2 2 2 2 2 2]\n",
      " [4 4 2 4 2 2 2 2 2 2 2 2]\n",
      " [4 4 5 4 2 2 2 2 2 2 2 2]\n",
      " [4 4 5 4 2 2 2 2 2 2 2 2]]\n",
      "epoch: 10, avg loss: 2.672368, avg accuracy: 0.542289\n",
      "\n",
      "[[ 4 24  5  9  2  2  2  2  2  2  2  2]\n",
      " [ 4 24  5  4  2  2  2  2  2  2  2  2]\n",
      " [ 4  5  4  4  2  2  2  2  2  2  2  2]\n",
      " [ 4  5  4  9  2  2  2  2  2  2  2  2]]\n",
      "epoch: 11, avg loss: 2.411274, avg accuracy: 0.607079\n",
      "\n",
      "[[ 11 619   2   9   2   2   2   2   2   2   2   2]\n",
      " [ 11   4   2   4   2   2   2   2   2   2   2   2]\n",
      " [ 11  24  39   4   2   2   2   2   2   2   2   2]\n",
      " [ 11   5  39  22   2   2   2   2   2   2   2   2]]\n",
      "epoch: 12, avg loss: 2.215441, avg accuracy: 0.661846\n",
      "\n",
      "[[ 11 619   2   9   2   2   2   2   2   2   2   2]\n",
      " [108  21   2   2   2   2   2   2   2   2   2   2]\n",
      " [108  24   2   2   2   2   2   2   2   2   2   2]\n",
      " [108  24   2   2   2   2   2   2   2   2   2   2]]\n",
      "epoch: 13, avg loss: 1.997225, avg accuracy: 0.726301\n",
      "\n",
      "[[ 11 619   2   2   2   2   2   2   2   2   2   2]\n",
      " [  4 619   2   2   2   2   2   2   2   2   2   2]\n",
      " [  4 619   2   2   2   2   2   2   2   2   2   2]\n",
      " [ 11   5   2   2   2   2   2   2   2   2   2   2]]\n",
      "epoch: 14, avg loss: 1.806813, avg accuracy: 0.770243\n",
      "\n",
      "[[ 11 619   2   2   2   2   2   2   2   2   2   2]\n",
      " [  4 619   2   2   2   2   2   2   2   2   2   2]\n",
      " [ 90   5   2   2   2   2   2   2   2   2   2   2]\n",
      " [ 90   5   2   2   2   2   2   2   2   2   2   2]]\n",
      "epoch: 15, avg loss: 1.636333, avg accuracy: 0.801631\n",
      "\n",
      "[[  4 619   2   9   2   2   2   2   2   2   2   2]\n",
      " [  4 619   4  22   2   2   2   2   2   2   2   2]\n",
      " [  4   4   4  22   2   2   2   2   2   2   2   2]\n",
      " [  4  24   4  22   2   2   2   2   2   2   2   2]]\n",
      "epoch: 16, avg loss: 1.469036, avg accuracy: 0.828742\n",
      "\n",
      "[[ 90 619   2   4   2   2   2   2   2   2   2   2]\n",
      " [ 90  24   4   4   2   2   2   2   2   2   2   2]\n",
      " [ 90   5   4   4   2   2   2   2   2   2   2   2]\n",
      " [ 90   5   4  22   2   2   2   2   2   2   2   2]]\n",
      "epoch: 17, avg loss: 1.322316, avg accuracy: 0.850024\n",
      "\n",
      "[[  4 619   2   4   2   2   2   2   2   2   2   2]\n",
      " [  4  24   2   4   2   2   2   2   2   2   2   2]\n",
      " [  4  24   4  12   2   2   2   2   2   2   2   2]\n",
      " [ 11   5   4  12   2   2   2   2   2   2   2   2]]\n",
      "epoch: 18, avg loss: 1.185861, avg accuracy: 0.858949\n",
      "\n",
      "[[ 90 619   2  12   2   2   2   2   2   2   2   2]\n",
      " [ 90 619  10  12   2   2   2   2   2   2   2   2]\n",
      " [ 90  24  12  63   2   2   2   2   2   2   2   2]\n",
      " [ 90   5  10  22   2   2   2   2   2   2   2   2]]\n",
      "epoch: 19, avg loss: 1.088775, avg accuracy: 0.870002\n",
      "\n",
      "[[ 11 619   2   9   2   2   2   2   2   2   2   2]\n",
      " [ 11  24   2  41   2   2   2   2   2   2   2   2]\n",
      " [ 11  24   4  41   2   2   2   2   2   2   2   2]\n",
      " [ 11   5   4  41   2   2   2   2   2   2   2   2]]\n",
      "epoch: 20, avg loss: 1.006287, avg accuracy: 0.874486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for k in range(0, len(short_questions), batch_size):\n",
    "        index = min(k+batch_size, len(short_questions))\n",
    "        batch_x, seq_x = pad_sentence_batch(X[k: index], PAD, maxlen_answer)\n",
    "        batch_y, seq_y = pad_sentence_batch(Y[k: index], PAD, maxlen_answer)\n",
    "        predicted, accuracy,loss, _ = sess.run([model.predicting_ids, \n",
    "                                                model.accuracy, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    total_loss /= (len(short_questions) / batch_size)\n",
    "    total_accuracy /= (len(short_questions) / batch_size)\n",
    "    print(predicted)\n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f\\n'%(i+1, total_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1\n",
      "QUESTION: i am a werewolf\n",
      "REAL ANSWER: a werewolf\n",
      "PREDICTED ANSWER: a werewolf what \n",
      "\n",
      "row 2\n",
      "QUESTION: i was dreaming again\n",
      "REAL ANSWER: i would think so\n",
      "PREDICTED ANSWER: a no okay \n",
      "\n",
      "row 3\n",
      "QUESTION: the kitchen\n",
      "REAL ANSWER: very nice\n",
      "PREDICTED ANSWER: a no i okay \n",
      "\n",
      "row 4\n",
      "QUESTION: the bedroom\n",
      "REAL ANSWER: there is only one bed\n",
      "PREDICTED ANSWER: a you i okay \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('QUESTION:',' '.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
    "    print('REAL ANSWER:',' '.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
    "    print('PREDICTED ANSWER:',' '.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1\n",
      "QUESTION: but david\n",
      "REAL ANSWER: is here that\n",
      "PREDICTED ANSWER: i you \n",
      "\n",
      "row 2\n",
      "QUESTION: hopeless it is hopeless\n",
      "REAL ANSWER: tell ballet then back\n",
      "PREDICTED ANSWER: i you \n",
      "\n",
      "row 3\n",
      "QUESTION: miss price\n",
      "REAL ANSWER: yes learning\n",
      "PREDICTED ANSWER: what that you \n",
      "\n",
      "row 4\n",
      "QUESTION: mr kessler wake up please\n",
      "REAL ANSWER: is here are\n",
      "PREDICTED ANSWER: what that you \n",
      "\n",
      "row 5\n",
      "QUESTION: there were witnesses\n",
      "REAL ANSWER: why she out\n",
      "PREDICTED ANSWER: what \n",
      "\n",
      "row 6\n",
      "QUESTION: what about it\n",
      "REAL ANSWER: not you are\n",
      "PREDICTED ANSWER: what you \n",
      "\n",
      "row 7\n",
      "QUESTION: go on ask them\n",
      "REAL ANSWER: i just home\n",
      "PREDICTED ANSWER: not you \n",
      "\n",
      "row 8\n",
      "QUESTION: beware the moon\n",
      "REAL ANSWER: seen hi is he\n",
      "PREDICTED ANSWER: i you \n",
      "\n",
      "row 9\n",
      "QUESTION: did you hear that\n",
      "REAL ANSWER: is down what\n",
      "PREDICTED ANSWER: i you \n",
      "\n",
      "row 10\n",
      "QUESTION: i heard that\n",
      "REAL ANSWER: it here not\n",
      "PREDICTED ANSWER: i yeah are \n",
      "\n",
      "row 11\n",
      "QUESTION: the hound of the baskervilles\n",
      "REAL ANSWER: heard\n",
      "PREDICTED ANSWER: i yeah are \n",
      "\n",
      "row 12\n",
      "QUESTION: it is moving\n",
      "REAL ANSWER: not you hear\n",
      "PREDICTED ANSWER: i is are \n",
      "\n",
      "row 13\n",
      "QUESTION: nice doggie good boy\n",
      "REAL ANSWER: bill stupid\n",
      "PREDICTED ANSWER: i the him are \n",
      "\n",
      "row 14\n",
      "QUESTION: it sounds far away\n",
      "REAL ANSWER: that pecos baby seen hi\n",
      "PREDICTED ANSWER: i are him see \n",
      "\n",
      "row 15\n",
      "QUESTION: debbie klein cried a lot\n",
      "REAL ANSWER: is will srai not\n",
      "PREDICTED ANSWER: i are am okay \n",
      "\n",
      "row 16\n",
      "QUESTION: what are you doing here\n",
      "REAL ANSWER: is know look i\n",
      "PREDICTED ANSWER: i are okay see made \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_x, seq_x = pad_sentence_batch(X_test[:batch_size], PAD, maxlen_question)\n",
    "batch_y, seq_y = pad_sentence_batch(Y_test[:batch_size], PAD, maxlen_answer)\n",
    "predicted = sess.run(model.predicting_ids, feed_dict={model.X:batch_x})\n",
    "\n",
    "for i in range(len(batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('QUESTION:',' '.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
    "    print('REAL ANSWER:',' '.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
    "    print('PREDICTED ANSWER:',' '.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
